# ğŸ›’ Instacart Data Warehouse Project

**Building a Data Warehouse for Instacart: Shopping Behavior Analysis and Product Recommendations**

---

## ğŸ“š Project Overview

- **Course:** Data Warehouse
- **Dataset:** Instacart Market Basket Analysis
  - 3.4M orders (3,124,301 records)
  - 33M order items (31,843,317 records)
  - 206K users (205,700 records)
  - 49K products (49,596 records)
  - 21 departments, 134 aisles
- **Tech Stack:** MariaDB 12.1, Python 3.13, Streamlit, scikit-learn, mlxtend
- **Architecture:** Constellation Schema with LIST + RANGE Partitioning
- **Total Size:** ~1.8 GB (with indexes)

---

## ğŸ“ Project Structure

```
Instacart/
â”œâ”€â”€ data/                       # CSV dataset files (681 MB)
â”œâ”€â”€ sql/                        # SQL scripts (11 files)
â”‚   â”œâ”€â”€ 01_create_database.sql
â”‚   â”œâ”€â”€ 02-06_dim_*.sql         # Dimension tables
â”‚   â”œâ”€â”€ 07-08_fact_*.sql        # Fact tables (partitioned)
â”‚   â””â”€â”€ 09-11_*.sql             # Indexes, checks, maintenance
â”œâ”€â”€ etl/                        # Python ETL Pipeline
â”‚   â”œâ”€â”€ config.py               # Database connection
â”‚   â”œâ”€â”€ load_dimensions.py      # Load dimension tables
â”‚   â”œâ”€â”€ load_facts.py           # Load fact tables
â”‚   â””â”€â”€ etl_pipeline.py         # Main orchestrator
â”œâ”€â”€ analysis/                   # SQL Analytical Queries (11 files)
â”œâ”€â”€ sql_results/                # Query results
â”œâ”€â”€ dashboard/                  # Streamlit Dashboard
â”‚   â”œâ”€â”€ app.py                  # Main app
â”‚   â””â”€â”€ pages/                  # 5 dashboard pages
â”œâ”€â”€ mining/                     # Data Mining Module
â”‚   â”œâ”€â”€ customer_clustering.py  # K-Means clustering
â”‚   â”œâ”€â”€ market_basket.py        # FP-Growth association rules
â”‚   â””â”€â”€ recommendation.py       # Hybrid recommender
â””â”€â”€ requirements.txt
```

---

## ğŸ—„ï¸ Database Schema

### Constellation Schema

**Dimension Tables:**
- `Dim_Time` (168 rows): Time dimension (7 days Ã— 24 hours)
- `Dim_User` (206K rows): Customer profiles with metrics
- `Dim_Product` (49K rows): Product catalog
- `Dim_Aisle` (134 rows): Product aisles
- `Dim_Department` (21 rows): Department hierarchy

**Fact Tables:**
- `Fact_Orders` (3.1M rows, 7 partitions): Orders with LIST partitioning by day of week
- `Fact_Order_Details` (31.8M rows, 8 partitions): Order items with RANGE partitioning by order_id

**Total:** 15 partitions, 36.6M rows, ~1.8 GB

---

## âš™ï¸ Configuration

```bash
# MariaDB Docker Container
Host: localhost:3307
Database: instacart_dwh
User: dwh_user
Password: dwh_pass123
```

**Start Docker Container:**
```bash
docker run -d \
  --name instacart-mariadb \
  -p 3307:3306 \
  -e MYSQL_ROOT_PASSWORD=rootpass \
  -e MYSQL_DATABASE=instacart_dwh \
  -e MYSQL_USER=dwh_user \
  -e MYSQL_PASSWORD=dwh_pass123 \
  mariadb:latest
```

**Environment Variables (`.env`):**
```
DB_HOST=localhost
DB_PORT=3307
DB_USER=dwh_user
DB_PASSWORD=dwh_pass123
DB_NAME=instacart_dwh
DATA_PATH=./data
```

---

## ğŸ”„ ETL Pipeline

### ETL Process
1. **Load Dimensions** â†’ 5 dimension tables
2. **Load Facts** â†’ 2 fact tables (chunked loading)
3. **Update References** â†’ Link time_id to Dim_Time
4. **Compute Metrics** â†’ Order metrics and user aggregations

### Performance
- **Total Records:** 36.6M rows
- **Total Time:** 30-45 minutes
- **Throughput:** ~1,380 rows/second
- **Chunk Size:** 50,000 rows per batch

### Run ETL
```bash
python etl/etl_pipeline.py
```

---

## ğŸ“Š SQL Analysis

### 11 Analytical Queries
1. **Top Products** - Best-selling products
2. **Peak Hours** - Busiest order times
3. **Day of Week** - Weekly trends
4. **Department Performance** - Sales by department
5. **Customer Segmentation** - User segments
6. **Aisle Reorder Analysis** - Reorder rates by aisle
7. **Basket Size Distribution** - Cart size patterns
8. **Weekend vs Weekday** - Period comparison
9. **Product Reorder Patterns** - Repurchase behavior
10. **Partition Performance** - Query optimization test
11. **Summary Statistics** - Overall metrics

### Key Insights
- ğŸ¥‡ **Top Product:** Banana (488K items, 84.48% reorder rate)
- ğŸ† **Top Department:** Produce (9.8M items, 65.04% reorder rate)
- ğŸ’ **VIP Customers:** 867 users (0.42%) with 50+ orders
- ğŸ¥› **Highest Reorder:** Milk aisle (78.18%)

### Run Analysis
```bash
./run_sql_analysis.sh
```

---

## ğŸ“ˆ Interactive Dashboard

### 5 Dashboard Pages
1. **Overview** - KPIs, trends, department breakdown
2. **Products** - Top products, aisle analysis
3. **Time Analysis** - Order heatmap, peak times
4. **Customers** - Segmentation, basket distribution
5. **Departments** - Sales volume, reorder comparison

### Features
- âœ… Real-time data from MariaDB
- âœ… Interactive Plotly charts
- âœ… Data caching for performance
- âœ… Responsive design

### Launch Dashboard
```bash
./run_dashboard.sh
# Access: http://localhost:8501
```

---

## ğŸ” Data Mining

### 3 Mining Modules

**1. Customer Clustering (K-Means)**
- 4 segments: VIP, Frequent, Regular, Occasional
- Features: Orders, basket size, reorder rate, days between orders
- Visualizations: PCA 2D/3D, elbow curve

**2. Market Basket Analysis (FP-Growth)**
- Association rules with min_support=0.01, min_confidence=0.3
- 500-2000 rules generated
- Metrics: Support, confidence, lift

**3. Product Recommendations (Hybrid)**
- Rule-based: Cart items â†’ Association rules
- Cluster-based: User segment â†’ Popular products
- Weighted combination: 60% rules + 40% cluster

### Run Data Mining
```bash
./run_mining.sh all
```

---

## ğŸš€ Quick Start

### 1. Start Database
```bash
docker start instacart-mariadb
```

### 2. Setup Python Environment
```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 3. Create Database Schema
```bash
cd sql
./run_all_sql.sh
```

### 4. Run ETL Pipeline
```bash
python etl/etl_pipeline.py
```

### 5. Launch Dashboard
```bash
./run_dashboard.sh
```

### 6. Run SQL Analysis
```bash
./run_sql_analysis.sh
```

### 7. Run Data Mining
```bash
./run_mining.sh all
```

---

## ğŸ“ˆ Results

### Performance
- âœ… **Partition Pruning:** 7x speedup for date-filtered queries
- âœ… **Query Time:** <1 second for most analytical queries
- âœ… **ETL Success Rate:** 100% (all records loaded)
- âœ… **Dashboard Load Time:** <2 seconds

### Deliverables
- âœ… Database Schema (7 tables, 15 partitions)
- âœ… ETL Pipeline (36.6M rows loaded)
- âœ… SQL Analysis (11 queries executed)
- âœ… Interactive Dashboard (5 pages)
- âœ… Data Mining (Clustering + Association Rules)

---

## ğŸ› ï¸ Tech Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| Database | MariaDB 12.1 | Data warehouse with partitioning |
| ETL | Python 3.13, Pandas, SQLAlchemy | Data pipeline |
| Analytics | SQL | Business intelligence queries |
| Dashboard | Streamlit, Plotly | Interactive visualization |
| Machine Learning | scikit-learn | K-Means clustering |
| Association Mining | mlxtend | FP-Growth algorithm |
| Container | Docker | MariaDB deployment |

---

## ğŸ”§ Troubleshooting

**Database Connection Issues:**
```bash
docker restart instacart-mariadb
docker logs instacart-mariadb
```

**ETL Errors:**
```bash
cat .env  # Check configuration
python -c "from etl.config import get_engine; print(get_engine())"
```

**Dashboard No Data:**
```bash
# Check if data is loaded
docker exec instacart-mariadb mariadb -u dwh_user -pdwh_pass123 instacart_dwh \
  -e "SELECT COUNT(*) FROM Fact_Orders;"
```

---

## ğŸ¯ Key Features

âœ… **Advanced Partitioning:** LIST + RANGE for performance optimization  
âœ… **Code-First Approach:** Python/SQL scripts (no GUI tools)  
âœ… **Interactive Dashboard:** 5 pages with Plotly visualizations  
âœ… **Advanced Analytics:** K-Means clustering + FP-Growth rules  
âœ… **Production-Ready:** Docker deployment, error handling  
âœ… **Comprehensive Analysis:** 11 SQL queries + Mining + Dashboard  

---

## ğŸ“Š Project Stats

- **Code Lines:** ~5,000+ (Python + SQL)
- **Database Tables:** 7 tables (2 facts + 5 dimensions)
- **Partitions:** 15 partitions
- **SQL Queries:** 11 analytical queries
- **Dashboard Pages:** 5 interactive pages
- **Mining Algorithms:** 2 (K-Means + FP-Growth)
- **Total Data:** 36.6M rows, ~1.8 GB

---

## ğŸ“ License

Educational project for Data Warehouse course.  
Dataset: [Instacart Market Basket Analysis](https://www.kaggle.com/c/instacart-market-basket-analysis) (Kaggle)

---

**Status:** âœ… **Project Complete**

```
Instacart/
â”œâ”€â”€ data/                       # Dataset CSV files (681 MB)
â”‚   â”œâ”€â”€ orders.csv
â”‚   â”œâ”€â”€ order_products__prior.csv
â”‚   â”œâ”€â”€ products.csv
â”‚   â”œâ”€â”€ departments.csv
â”‚   â”œâ”€â”€ aisles.csv
â”‚   â””â”€â”€ users.csv
â”‚
â”œâ”€â”€ sql/                        # SQL scripts (11 files)
â”‚   â”œâ”€â”€ 01_create_database.sql
â”‚   â”œâ”€â”€ 02_dim_time.sql
â”‚   â”œâ”€â”€ 03_dim_department.sql
â”‚   â”œâ”€â”€ 04_dim_aisle.sql
â”‚   â”œâ”€â”€ 05_dim_product.sql
â”‚   â”œâ”€â”€ 06_dim_user.sql
â”‚   â”œâ”€â”€ 07_fact_orders.sql      # LIST partitioning (7 partitions)
â”‚   â”œâ”€â”€ 08_fact_order_details.sql  # RANGE partitioning (8 partitions)
â”‚   â”œâ”€â”€ 09_additional_indexes.sql
â”‚   â”œâ”€â”€ 10_check_partitions.sql
â”‚   â”œâ”€â”€ 11_maintenance.sql
â”‚   â””â”€â”€ run_all_sql.sh
â”‚
â”œâ”€â”€ etl/                        # Python ETL Pipeline (4 scripts)
â”‚   â”œâ”€â”€ config.py               # Database connection config
â”‚   â”œâ”€â”€ load_dimensions.py      # Load 5 dimension tables
â”‚   â”œâ”€â”€ load_facts.py           # Load 2 fact tables
â”‚   â”œâ”€â”€ update_time_id.py       # Update time_id references
â”‚   â””â”€â”€ etl_pipeline.py         # Main orchestrator
â”‚
â”œâ”€â”€ analysis/                   # SQL Analytical Queries (11 files)
â”‚   â”œâ”€â”€ 01_top_products.sql
â”‚   â”œâ”€â”€ 02_peak_hours.sql
â”‚   â”œâ”€â”€ 03_day_of_week.sql
â”‚   â”œâ”€â”€ 04_department_performance.sql
â”‚   â”œâ”€â”€ 05_customer_segmentation.sql
â”‚   â”œâ”€â”€ 06_aisle_reorder_analysis.sql
â”‚   â”œâ”€â”€ 07_basket_size_distribution.sql
â”‚   â”œâ”€â”€ 08_weekend_vs_weekday.sql
â”‚   â”œâ”€â”€ 09_product_reorder_patterns.sql
â”‚   â”œâ”€â”€ 10_partition_performance_test.sql
â”‚   â”œâ”€â”€ 11_summary_statistics.sql
â”‚   â””â”€â”€ run_sql_analysis.sh      # Run all queries
â”‚
â”œâ”€â”€ sql_results/                # SQL Analysis Results (11 files)
â”‚   â”œâ”€â”€ 01_top_products.txt
â”‚   â”œâ”€â”€ 02_peak_hours.txt
â”‚   â””â”€â”€ ... (9 more files)
â”‚
â”œâ”€â”€ dashboard/                  # Streamlit Interactive Dashboard
â”‚   â”œâ”€â”€ app.py                  # Main app with routing
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ overview.py         # KPI cards & overall trends
â”‚   â”‚   â”œâ”€â”€ products.py          # Product analytics
â”‚   â”‚   â”œâ”€â”€ time_analysis.py     # Temporal patterns
â”‚   â”‚   â”œâ”€â”€ customers.py         # Customer segmentation
â”‚   â”‚   â””â”€â”€ departments.py       # Department performance
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ mining/                     # Data Mining Module
â”‚   â”œâ”€â”€ customer_clustering.py  # K-Means clustering
â”‚   â”œâ”€â”€ market_basket.py        # FP-Growth association rules
â”‚   â”œâ”€â”€ recommendation.py       # Hybrid recommender system
â”‚   â”œâ”€â”€ results/                # Output files
â”‚   â”‚   â”œâ”€â”€ cluster_profiles.csv
â”‚   â”‚   â”œâ”€â”€ association_rules.csv
â”‚   â”‚   â””â”€â”€ *.png (visualizations)
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ run_complete.sh             # Master script (setup â†’ dashboard)
â”œâ”€â”€ run_dashboard.sh            # Launch dashboard
â”œâ”€â”€ run_mining.sh               # Run data mining
â”œâ”€â”€ run_sql_analysis.sh         # Run SQL analysis
â””â”€â”€ README.md                   # This file

**Status:** âœ… **Project Complete**
